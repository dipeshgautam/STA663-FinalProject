\documentclass{article}
\usepackage{float}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage[margin=1in]{geometry}
\usepackage{caption}
\usepackage{booktabs}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyvrb}
\usepackage{graphicx}
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}%
{fontsize=\footnotesize,
 %
 frame=lines,  % top and bottom rule only
 framesep=2em, % separation between frame and text
 rulecolor=\color{Gray},
 %
 label=\fbox{\color{Black}profiler.txt},
 labelposition=topline,
 %
 commandchars=\|\(\), % escape character and argument delimiters for
                      % commands within the verbatim
 commentchar=*        % comment character
}

\begin{document}
\title{STA663 Final Project \\ Infinite Latent Feature Models and the Indian Buffet Process}
\author{Dipesh Gautam}
\date{\today}
\maketitle

\section{Indian Buffet Process (IBP)}

The Indian Buffet is an adaptation of Chinese Buffett Process where each object instead of being associated with a single latent class can  be associated with multiple classes. This is particularly useful when each object has multile latent features and by associating objects with a single class we cannot partition them into homogeneous subsets.

In the Indian buffet process, $N$ customers enter a restaurant one after another. Each customer encounters a buffet 
consisting of infinitely many dishes arranged in a line. The first customer starts at the left of the buffet and 
takes a serving from each dish, stopping after a Poisson($\alpha$) number of dishes. The $i$th customer moves along the buffet, 
sampling dishes in proportion to their popularity, taking dish $k$ with probability $\frac{m_k}{i}$ , where $m_k$ is the number of 
previous customers who have sampled that dish. Having reached the end of all previous sampled dishes, the $i$th customer 
then tries a Poisson($\frac{\alpha}{i}$) number of new dishes. Which costumer chose which dishes is indicated using a binary matrix \textbf{Z} with $N$ rows and infinitely many columns(corresponding to the infinitely many selection of dished), where $z_{ik}$ = 1 if the $i$th costumer sampled $k$th dish.

IBP can be used as a prior in models for unsupervised learning. An example of which is presentd in the paper by Griffiths and Ghahramani, where IBP is used as a prior in linear-Gaussian binary latent feature model.


\section{Algorithm}

\begin{itemize}
\item{Gamma prior for $\alpha$
$$
\alpha \sim Gamma(1,1)
$$}
\item{Prior on \textbf{Z} is obtained by IBP as:
$$
P(z_{ik}=1|\textbf{z}_{-i,k}) = \frac{n_{-i,k}}{N}
$$}

\item{Likelihood is given by
\begin{equation}
P(X|Z,\sigma_X, \sigma_A) = \frac{1}{(2 \pi)^{ND/2} (\sigma_X)^{(N-K)D}(\sigma_A)^{KD}(|Z^TZ+\frac{\sigma_X^2}{\sigma_A^2}I|)^{D/2}}exp\{-\frac{1}{2\sigma_X^2}tr(X^T(I-Z(Z^TZ+\frac{\sigma_X^2}{\sigma_A^2}I)^{-1}Z^T)X)\}
\end{equation}
}
After we have the likelihood and the prior given by IBP,
\item{full conditional posterior for \textbf{Z} can be calculated as:
$$
P(z_{ik}|X,Z_{-(i,k)},\sigma_X,\sigma_A) \propto  P(X|Z,\sigma_X, \sigma_A) * P(z_{ik}=1|\textbf{z}_{-i,k}) 
$$
}

To sample the number of new features for observation $i$, we use a truncated distribution, computing probabilities for a range of values $K_1^{(i)}$ up to an upper bound (say 4). The prior on number of features is given by $Poisson(\frac{\alpha}{N})$.
Using this prior and the likelihood, we sample the nummber of new features.

\item{Full conditional posterior for $\alpha$ is given by:
$$
P(\alpha|Z) \sim Gamma(1+K_+,1+\sum_{i=1}^{N} H_i)
$$}

\item{For $\sigma_X$ and $\sigma_A$, we use MH algorithm as follows:
\begin{eqnarray}
\epsilon \sim Uniform(-.05,.05)\\
\sigma_X^{*} =  \sigma_X +\epsilon\\
\end{eqnarray}
Accept this new $\sigma_X$ with probability given by:
$$
AR = min\{1,\frac{Likelihood(X|\sigma_X^{\*},...)}{Likelihood(X|\sigma_X,...)}\}\\
$$
Where AR is the acceptance ratio. We use similar algorithm to sample $\sigma_A$}

\end{itemize}



\section{Profiling}
We profiled the code using \textit{cProfile} to figure out the bottleneck. The result is shown in \textit{profiler.txt}. We see that most of the computational time is spent on calculating the \textit{log likelihood(ll)}  and matrix inversion. Due to this fact, one of the first things we looked at were ways to reduce computation time for likelihood and/or inverse calculation.\\

\VerbatimInput{profiler.txt}

\subsection{Improved matrix Inversion}
We tried the matrix inversion method described in Griffiths and Ghahramani(2005, eq 51-54), where the method reduces the runtime by allowing us to perform rank one updates instead when only one value is changed. We implemented the algorithm and were able to speed up the process as shown in Table \ref{compareInverse}.

\begin{table}[ht]
\centering
\caption{Comparision of matrix inverse methods \label{compareInverse}}
\input{latex_tables/inverseMethods}
\end{table}

Even though we were able to improve the performance, due to some numerical errors, we were not able to obtain a stationary MCMC chain using this method. This could be achieved by spending some more time on it but due to lack of time, we had to abandon this method and move on.

\subsection{Improved likelihood function}



\begin{table}[ht]
\centering
\caption{Runtime Comparision \label{runtimes}}
\input{latex_tables/Runtimes}

\end{table}



\begin{figure}
\caption {Original Features and First four simulated objects}
\includegraphics[width=\linewidth]{figures/Original.png}
\label{fig:original}
\end{figure}



\begin{figure}
\caption {Features Detected after MCMC and First four recreated objects}
\includegraphics[width=\linewidth]{figures/Detected.png}
\label{fig:detected}
\end{figure}

\begin{figure}
\caption {Traceplots for $\sigma_X$, $\sigma_A$ and $\alpha$ after burn-in}
\includegraphics[width=\linewidth]{figures/Trace.png}
\label{fig:trace}
\end{figure}

\begin{figure}
\caption {Distribution of Kplus}
\includegraphics[width=\linewidth]{figures/kDistribution.png}
\label{fig:dist}
\end{figure}

\begin{table}[ht]
\centering
\caption{Runtime Comparision \label{runtimes}}
\input{latex_tables/featuresDetected}

\end{table}





\end{document}
