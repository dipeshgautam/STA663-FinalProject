{
 "metadata": {
  "name": "",
  "signature": "sha256:6d9e8c1987d44335c63c33d62042f6ce00051537e1d5d453dda7bab2f088b9d1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation of Infinite Latent Feature Models and the Indian Buffet Process"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Indian Buffet Process (IBP)**\n",
      "\n",
      "The Indian Buffet is an adaptation of Chinese Buffett Process where each object instead of being associated with a single latent class can  be associated with multiple classes. This is particularly useful when each object has multile latent features and by associating objects with a single class we cannot partition them into homogeneous subsets.\n",
      "\n",
      "In the Indian buffet process, $N$ customers enter a restaurant one after another. Each customer encounters a buffet \n",
      "consisting of infinitely many dishes arranged in a line. The first customer starts at the left of the buffet and \n",
      "takes a serving from each dish, stopping after a Poisson($\\alpha$) number of dishes. The $i$th customer moves along the buffet, \n",
      "sampling dishes in proportion to their popularity, taking dish $k$ with probability $\\frac{m_k}{i}$ , where $m_k$ is the number of \n",
      "previous customers who have sampled that dish. Having reached the end of all previous sampled dishes, the $i$th customer \n",
      "then tries a Poisson($\\frac{\\alpha}{i}$) number of new dishes. Which costumer chose which dishes is indicated using a binary matrix **Z** with $N$ rows and infinitely many columns(corresponding to the infinitely many selection of dished), where $z_{ik}$ = 1 if the $i$th costumer sampled $k$th dish.\n",
      "\n",
      "IBP can be used as a prior in models for unsupervised learning. An example of which is presentd in the paper by Griffiths and Ghahramani, where IBP is used as a prior in linear-Gaussian binary latent feature model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Gamma prior for $\\alpha$\n",
      "$$\n",
      "\\alpha \\sim Gamma(1,1)\n",
      "$$\n",
      "* Prior on **Z** is obtained by IBP as:\n",
      "$$\n",
      "P(z_{ik}=1|\\textbf{z}_{-i,k}) = \\frac{n_{-i,k}}{N}\n",
      "$$\n",
      "\n",
      "* Likelihood is given by\n",
      "\\begin{equation}\n",
      "P(X|Z,\\sigma_X, \\sigma_A) = \\frac{1}{(2 \\pi)^{ND/2} (\\sigma_X)^{(N-K)D}(\\sigma_A)^{KD}(|Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I|)^{D/2}}exp\\{-\\frac{1}{2\\sigma_X^2}tr(X^T(I-Z(Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I)^{-1}Z^T)X)\\}\n",
      "\\end{equation}\n",
      "\n",
      "After we have the likelihood and the prior given by IBP,\n",
      "\n",
      "* full conditional posterior for **Z** can be calculated as:\n",
      "$$\n",
      "P(z_{ik}|X,Z_{-(i,k)},\\sigma_X,\\sigma_A) \\propto  P(X|Z,\\sigma_X, \\sigma_A) * P(z_{ik}=1|\\textbf{z}_{-i,k}) \n",
      "$$\n",
      "\n",
      "To sample the number of new features for observation $i$, we use a truncated distribution, computing probabilities for a range of values $K_1^{(i)}$ up to an upper bound (say 4). The prior on number of features is given by $Poisson(\\frac{\\alpha}{N})$.\n",
      "Using this prior and the likelihood, we sample the nummber of new features.\n",
      "\n",
      "* Full conditional posterior for $\\alpha$ is given by:\n",
      "$$\n",
      "P(\\alpha|Z) \\sim Gamma(1+K_+,1+\\sum_{i=1}^{N} H_i)\n",
      "$$\n",
      "\n",
      "* For $\\sigma_X$ and $\\sigma_A$, we use MH algorithm as follows:\n",
      "\\begin{eqnarray}\n",
      "\\epsilon \\sim Uniform(-.05,.05)\\\\\n",
      "\\sigma_X^{*} =  \\sigma_X +\\epsilon\\\\\n",
      "\\end{eqnarray}\n",
      "Accept this new $\\sigma_X$ with probability given by:\n",
      "$$\n",
      "AR = min\\{1,\\frac{Likelihood(X|\\sigma_X^{*},...)}{Likelihood(X|\\sigma_X,...)}\\}\\\\\n",
      "$$\n",
      "Where AR is the acceptance ratio. We use similar algorithm to sample $\\sigma_A$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Pseudo-code for a single iteration of Gibbs and MH combined algorigthm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "3.141592653589793"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#N = number of objects\n",
      "#K = current number of features with at least one object\n",
      "\n",
      "# define a likelihood function to be used multiple times later, try to implement it in C or at change to log likelihood\n",
      "def ll(X, Z, sigma_A, sigma_X, K, D, N):\n",
      "    M = Z.T.dot(Z)+sigma_X**2/sigma_A**2*np.identity(K)\n",
      "    return -np.log(2*np.pi)*N*D*.5-log(sigma_X)*(N-K)*D-log(sigma_A)*K*D-log(det(M)) \\ \n",
      "        -.5/sigma_X^2+np.matrix.trace( X.T.dot( np.identity(N)-Z.dot(np.linalg.inv(M).dot(Z.T)) ).dot(X) )\n",
      "\n",
      "#update z\n",
      "for i in range(N):\n",
      "    for k in range(K):\n",
      "        #set Z[i,k] = 0 and calculate posterior probability\n",
      "        Z[i,k] = 0\n",
      "        p0 = ll(X, Z,sigma_X, sigma_A, K, D, N) + P(z_ik=1|z_(-i,k))\n",
      "        #set Z[i,k] = 1 and calculate posterior probability\n",
      "        Z[i,k] = 1\n",
      "        p1 = ll(X, Z,sigma_X, sigma_A, K, D, N)*P(z_ik=1|z_(-i,k))\n",
      "        \n",
      "        U = np.random.uniform(0,1)\n",
      "        if U<p1/(p0+p1):\n",
      "            Z[i,k] = 1\n",
      "        else:\n",
      "            Z[i,k] = 0\n",
      "            \n",
      "    #Sample number of new features\n",
      "    prob = numpy.zeros(4)\n",
      "    for k_new in range(3): # max new features is 4\n",
      "        Z_temp = Z\n",
      "        Z[i,K+1:K+k_new] = 1 # add appropriate columns to Z_temp and set all possible new features to 1        \n",
      "        prob[k_new] = poisson(k,alpha/N) * likelihood(Z_temp, K=K+k_new...)\n",
      "    #normalize prob\n",
      "    prob = prob/sum(prob)\n",
      "    \n",
      "    U = uniform(0,1)\n",
      "    p = 0\n",
      "    for k_new in range(3):\n",
      "        p = p+prob[k_new]\n",
      "        if U<p:\n",
      "            k_new_i = k_new\n",
      "            break\n",
      "     \n",
      "    #set all the added features as 1\n",
      "    #also change the size of z to reflect this change.\n",
      "    Z[i,k+1:k+k_new] = 1\n",
      "    K = K+k_new_i \n",
      "\n",
      "#update sigma_A\n",
      "eps = uniform(-.05,.05)\n",
      "sigma_X_new = sigma_X+eps\n",
      "likelihood_current = likelihood(sigma_X,...)\n",
      "likelihood_new = likelihood(sigma_X_new,...)\n",
      "\n",
      "AR_X = min(1,likelihood_new/likelihood_current)\n",
      "\n",
      "U = uniform(0,1)\n",
      "if U < AR_X:\n",
      "    sigma_X = sigma_X_new\n",
      "    \n",
      "    \n",
      "#update sigma_A\n",
      "eps = uniform(-.05,.05)\n",
      "sigma_A_new = sigma_A + eps\n",
      "\n",
      "likelihood_current = likelihood(sigma_A,...)\n",
      "likelihood_new = likelihood(sigma_A_new,...)\n",
      "\n",
      "AR_A = min(1,likelihood_new/likelihood_current)\n",
      "\n",
      "U = uniform(0,1)\n",
      "if U < AR_A:\n",
      "    sigma_A = sigma_A_new\n",
      "\n",
      "#update alpha\n",
      "alpha = sample_gamma(1+K, 1+harmonic_mean_N)        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Unit testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To check if the code is working properly some of the unit testings I've come up with so far are:\n",
      "* Probabilities calculated for the presence of feature have to be between 0 and 1.\n",
      "* "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}